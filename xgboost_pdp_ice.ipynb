{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License \n",
    "\n",
    "Copyright 2017 Patrick Hall and the H2O.ai team\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering Transparency into Your Machine Learning Model with Python and XGBoost\n",
    "#### Monotonic XGBoost models, partial dependence, and ICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key to building interpretable models is to limit their complexity. The more complex a model is, the harder it is to explain and understand. Overly complex models can also make unstable predictions on new data, which is both difficult to explain and makes models harder to trust. Monotonicity constraints not only simplify models, but do so in a way that is somewhat natural for human reasoning, increasing the transparency of predictive models. Under monotonicity constraints, model predictions can only increase or only decrease as an input variable value increases, and the direction of the constraint is typically specified by the user for logical reasons. For instance, a model might be constrained to produce only increasing probabilities of a certain medical condition as a patient's age increases, or to make only increasing predictions for home prices as a home's square footage increases. \n",
    "\n",
    "In this notebook a gradient boosting machine (GBM) is trained with monotonicity constraints to predict credit card payment defaults, using the UCI credit card default data, Python, NumPy, Pandas, and XGBoost. First, the credit card default data is loaded and prepared. Then Pearson correlation with the prediction target is used to determine the direction of the monotonicity constraints for each input variable and the model is trained. After the model is trained, partial dependence and individual conditional expectation (ICE) plots are used to analyze and verify the model's monotonic behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with Python package imports. NumPy is used for basic arrray, vector, and matrix calculations. Pandas is used for data frame manipulation and plotting, and XGBoost is used to train a GBM with monotonicity constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np                   # array, vector, matrix calculations\n",
    "import pandas as pd                  # DataFrame handling\n",
    "import xgboost as xgb                # gradient boosting machines (GBMs)\n",
    "\n",
    "import matplotlib.pyplot as plt      # plotting\n",
    "pd.options.display.max_columns = 999 # enable display of all columns in notebook\n",
    "\n",
    "# enables display of plots in notebook\n",
    "%matplotlib inline              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download, explore, and prepare UCI credit card default data\n",
    "\n",
    "UCI credit card default data: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "The UCI credit card default data contains demographic and payment information about credit card customers in Taiwan in the year 2005. The data set contains 23 input variables: \n",
    "\n",
    "* **`LIMIT_BAL`**: Amount of given credit (NT dollar)\n",
    "* **`SEX`**: 1 = male; 2 = female\n",
    "* **`EDUCATION`**: 1 = graduate school; 2 = university; 3 = high school; 4 = others \n",
    "* **`MARRIAGE`**: 1 = married; 2 = single; 3 = others\n",
    "* **`AGE`**: Age in years \n",
    "* **`PAY_0`, `PAY_2` - `PAY_6`**: History of past payment; `PAY_0` = the repayment status in September, 2005; `PAY_2` = the repayment status in August, 2005; ...; `PAY_6` = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "* **`BILL_AMT1` - `BILL_AMT6`**: Amount of bill statement (NT dollar). `BILL_AMNT1` = amount of bill statement in September, 2005; `BILL_AMT2` = amount of bill statement in August, 2005; ...; `BILL_AMT6` = amount of bill statement in April, 2005. \n",
    "* **`PAY_AMT1` - `PAY_AMT6`**: Amount of previous payment (NT dollar). `PAY_AMT1` = amount paid in September, 2005; `PAY_AMT2` = amount paid in August, 2005; ...; `PAY_AMT6` = amount paid in April, 2005. \n",
    "\n",
    "These 23 input variables are used to predict the target variable, whether or not a customer defaulted on their credit card bill in late 2005. Because XGBoost accepts only numeric inputs, all variables will be treated as numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data and clean\n",
    "The credit card default data is available as an `.xls` file. Pandas reads `.xls` files automatically, so it's used to load the credit card default data and give the prediction target a shorter name: `DEFAULT_NEXT_MONTH`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import XLS file\n",
    "path = 'default_of_credit_card_clients.xls'\n",
    "data = pd.read_excel(path,\n",
    "                     skiprows=1) # skip the first row of the spreadsheet\n",
    "\n",
    "# remove spaces from target column name \n",
    "data = data.rename(columns={'default payment next month': 'DEFAULT_NEXT_MONTH'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign modeling roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shorthand name `y` is assigned to the prediction target. `X` is assigned to all other input variables in the credit card default data except the row indentifier, `ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign target and inputs for GBM\n",
    "y = 'DEFAULT_NEXT_MONTH'\n",
    "X = [name for name in data.columns if name not in [y, 'ID']]\n",
    "print('y =', y)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display descriptive statistics\n",
    "The Pandas `describe()` function displays a brief description of the credit card default data. The input variables `SEX`, `EDUCATION`, `MARRIAGE`, `PAY_0`-`PAY_6`, and the prediction target `DEFAULT_NEXT_MONTH`, are really categorical variables, but they have already been encoded into meaningful numeric, integer values, which is great for XGBoost. Also, there are no missing values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[X + [y]].describe() # display descriptive statistics for all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Investigate pair-wise Pearson correlations for DEFAULT_NEXT_MONTH\n",
    "\n",
    "Monotonic relationships are much easier to explain to colleagues, bosses, customers, and regulators than more complex, non-monotonic relationships and monotonic relationships may also prevent overfitting and excess error due to variance for new data.\n",
    "\n",
    "To train a transparent monotonic classifier, contraints must be supplied to XGBoost that determine whether the learned relationship between an input variable and the prediction target `DEFAULT_NEXT_MONTH` will be increasing for increases in an input variable or decreasing for increases in an input variable. Pearson correlation provides a linear measure of the direction of the relationship between each input variable and the target. If the pair-wise Pearson correlation between an input and `DEFAULT_NEXT_MONTH` is positive, it will be constrained to have an increasing relationship with the predictions for `DEFAULT_NEXT_MONTH`. If the pair-wise Pearson correlation is negative, the input will be constrained to have a decreasing relationship with the predictions for `DEFAULT_NEXT_MONTH`. \n",
    "\n",
    "Constrainsts are supplied to XGBoost in the form of a Python tuple with length equal to the number of inputs. Each item in the tuple is associated with an input variable based on its index in the tuple. The first constraint in the tuple is associated with the first variable in the training data, the second constraint in the tuple is associated with the second variable in the training data, and so on. The constraints themselves take the form of a 1 for a positive relationship and a -1 for a negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pearson correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas `.corr()` function returns the pair-wise Pearson correlation between variables in a Pandas DataFrame. Because `DEFAULT_NEXT_MONTH` is the last column in the `data` DataFrame, the last column of the Pearson correlation matrix indicates the direction of the linear relationship between each input variable and the prediction target, `DEFAULT_NEXT_MONTH`. According to the calculated values, as a customer's balance limit (`LIMIT_BAL`), bill amounts (`BILL_AMT1`-`BILL_AMT6`), and payment amounts (`PAY_AMT1`-`PAY_AMT6`) increase, their probability of default tends to decrease. However as a customer's number of late payments increase (`PAY_0`, `PAY_2`-`PAY6`), their probability of default usually increases. In general, the Pearson correlation values make sense, and they will be used to ensure that the modeled relationships will make sense as well. (Pearson correlation values between the target variable, DEFAULT_NEXT_MONTH, and each input variable are displayed directly below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# displays last column of Pearson correlation matrix as Pandas DataFrame\n",
    "pd.DataFrame(data[X + [y]].corr()[y]).iloc[:-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Create tuple of monotonicity constraints from Pearson correlation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column of the Pearson correlation matrix is transformed from a numeric column in a Pandas DataFrame into a Python tuple of `1`s and `-1`s that will be used to specifiy monotonicity constraints for each input variable in XGBoost. If the Pearson correlation between an input variable and `DEFAULT_NEXT_MONTH` is positive, a positive montonic relationship constraint is specified for that variable using `1`. If the correlation is negative, a negative monotonic constraint is specified using `-1`. (Specifying `0` indicates that no constraints should be used.) The resulting tuple will be passed to XGBoost when the GBM model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates a tuple in which positive correlation values are assigned a 1\n",
    "# and negative correlation values are assigned a -1\n",
    "mono_constraints = tuple([int(i) for i in np.sign(data[X + [y]].corr()[y].values[:-1])])\n",
    "\n",
    "# (-1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train XGBoost with monotonicity constraints\n",
    "\n",
    "XGBoost is a very accurate, open source GBM library for regression and classification tasks. XGBoost can learn complex relationships between input variables and a target variable, but here the `monotone_constraints` tuning parameter is used to enforce monotonicity between inputs and the prediction for `DEFAULT_NEXT_MONTH`. XGBoost's early stopping functionality is also used to limit overfitting to the training data\n",
    "\n",
    "XGBoost is available from: https://github.com/dmlc/xgboost and the implementation of XGBoost is described in detail here: http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf.\n",
    "\n",
    "After training, GBM variable importance is calculated and displayed. GBM variable importance is a global measure of the overall impact of an input variable on the GBM model predictions. Global variable importance values give an indication of the magnitude of a variable's contribution to model predictions for all observations. To enhance trust in the GBM model, variable importance values should typically conform to human domain knowledge and reasonable expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and test sets for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit card default data is split into training and test sets to monitor and prevent overtraining. Reproducibility is another important factor in creating trustworthy models, and randomly splitting datasets can introduce randomness in model predictions and other results. A random seed is used here to ensure the data split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345) # set random seed for reproducibility\n",
    "split_ratio = 0.7     # 70%/30% train/test split\n",
    "\n",
    "# execute split\n",
    "split = np.random.rand(len(data)) < split_ratio\n",
    "train = data[split]\n",
    "test = data[~split]\n",
    "\n",
    "# summarize split\n",
    "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
    "print('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train XGBoost GBM classifier\n",
    "To train an XGBoost classifier, the training and test data must be converted from Pandas DataFrames into SVMLight format. The `DMatrix()` function in the XGBoost package is used to convert the data. Many XGBoost tuning parameters must be specified as well. Typically a grid search would be performed to identify the best parameters for a given modeling task. For brevity's sake, a previously-discovered set of good tuning parameters are specified here. Notice that the monotonicity constraints are passed to XGBoost using the `monotone_constraints` parameter. Because gradient boosting methods typically resample training data, an additional random seed is also specified for XGBoost using the `seed` paramter to create reproducible predictions, error rates, and variable importance values. To avoid overfitting, the `early_stopping_rounds` parameter is used to stop the training process after the test area under the curve (AUC) statistic fails to increase for 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGBoost uses SVMLight data structure, not Numpy arrays or Pandas DataFrames \n",
    "dtrain = xgb.DMatrix(train[X], train[y])\n",
    "dtest = xgb.DMatrix(test[X], test[y])\n",
    "\n",
    "# used to calibrate predictions to mean of y \n",
    "base_y = train[y].mean()\n",
    "\n",
    "# tuning parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',             # produces 0-1 probabilities for binary classification\n",
    "    'booster': 'gbtree',                        # base learner will be decision tree\n",
    "    'eval_metric': 'auc',                       # stop training based on maximum AUC, AUC always between 0-1\n",
    "    'eta': 0.08,                                # learning rate\n",
    "    'subsample': 0.9,                           # use 90% of rows in each decision tree\n",
    "    'colsample_bytree': 0.9,                    # use 90% of columns in each decision tree\n",
    "    'max_depth': 15,                            # allow decision trees to grow to depth of 15\n",
    "    'monotone_constraints':mono_constraints,    # 1 = increasing relationship, -1 = decreasing relationship\n",
    "    'base_score': base_y,                       # calibrate predictions to mean of y \n",
    "    'seed': 12345                               # set random seed for reproducibility\n",
    "}\n",
    "\n",
    "# watchlist is used for early stopping\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "# train model\n",
    "xgb_model = xgb.train(params,                   # set tuning parameters from above                   \n",
    "                      dtrain,                   # training data\n",
    "                      1000,                     # maximum of 1000 iterations (trees)\n",
    "                      evals=watchlist,          # use watchlist for early stopping \n",
    "                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in AUC\n",
    "                      verbose_eval=True)        # display iteration progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display variable importance\n",
    "The variable importance ranking should be parsimonious with human domain knowledge and reasonable expectations. In this case, `LIMIT_BAL` is by far the most important variable. As credit limits are a tool used to price risk into lending decisions, this result appears valid.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot variable importance bar chart\n",
    "_ = xgb.plot_importance(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculating partial dependence and ICE to validate and explain monotonic behavior\n",
    "\n",
    "Partial dependence plots are used to view the global, average prediction behavior of a variable under the monotonic model. Partial dependence plots show the average prediction of the monotonic model as a function of specific values of an input variable of interest, indicating how the monotonic GBM predictions change based on the values of the input variable of interest, while taking nonlinearity into consideration and averaging out the effects of all other\n",
    "input variables. Partial dependence plots enable increased transparency into the monotonic GBM's mechanisms and enable validation and debugging of the monotonic GBM by comparing a variable's average predictions across its domain to known standards and reasonable expectations. Partial dependence plots are described in greater detail in *The Elements of Statistical Learning*, section 10.13: https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf.\n",
    "\n",
    "Individual conditional expectation (ICE) plots, a newer and less well-known adaptation of partial dependence plots, can be used to create more localized explanations for a single observation of data using the same basic ideas as partial dependence plots. ICE is also a type of nonlinear sensitivity analysis in which the model predictions for a single observation are measured while a feature of interest is varied over its domain. ICE increases understanding and transparency by displaying the nonlinear behavior of the monotonic GBM. ICE also enhances trust, accountability, and fairness by enabling comparisons of nonlinear behavior to human domain knowledge and reasonable expectations. ICE, as a type of sensitivity analysis, can also engender trust when model behavior on simulated or extreme data points is acceptable. A detailed description of ICE is available in this arXiv preprint: https://arxiv.org/abs/1309.6392.\n",
    "\n",
    "Because partial dependence and ICE are measured on the same scale, they can be displayed in the same line plot to compare the global, average prediction behavior for the entire model and the local prediction behavior for certain rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for calculating partial dependence\n",
    "Since partial dependence and ICE will be calculated for several important variables in the GBM model, it's convenient to have a function doing so. It's probably best to analyze partial dependence and ICE for all variables in a model, but only the top three most important input variables will be investigated here. It's also a good idea to analyze partial dependence and ICE on the test data, or other holdout datasets, to see how the model will perform on new data. \n",
    "This simple function is designed to return partial dependence when it is called for an entire dataset and ICE when it is called for a single row. The `bins` argument will be used later to calculate ICE values at the same places in an input variable domain that partial dependence is calculated directly below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def par_dep(xs, frame, model, resolution=20, bins=None):\n",
    "    \n",
    "    \"\"\" Creates Pandas DataFrame containing partial dependence for a \n",
    "        single variable.\n",
    "    \n",
    "    Args:\n",
    "        xs: Variable for which to calculate partial dependence.\n",
    "        frame: Pandas DataFrame for which to calculate partial dependence.\n",
    "        model: XGBoost model for which to calculate partial dependence.\n",
    "        resolution: The number of points across the domain of xs for which \n",
    "                    to calculate partial dependence, default 20.\n",
    "        bins: List of values at which to set xs, default 20 equally-spaced \n",
    "              points between column minimum and maximum.\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame containing partial dependence values.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # turn off pesky Pandas copy warning\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    # initialize empty Pandas DataFrame with correct column names\n",
    "    par_dep_frame = pd.DataFrame(columns=[xs, 'partial_dependence'])\n",
    "    \n",
    "    # cache original column values \n",
    "    col_cache = frame.loc[:, xs].copy(deep=True)\n",
    "  \n",
    "    # determine values at which to calculate partial dependence\n",
    "    if bins == None:\n",
    "        min_ = frame[xs].min()\n",
    "        max_ = frame[xs].max()\n",
    "        by = (max_ - min_)/resolution\n",
    "        bins = np.arange(min_, max_, by)\n",
    "        \n",
    "    # calculate partial dependence  \n",
    "    # by setting column of interest to constant \n",
    "    # and scoring the altered data and taking the mean of the predictions\n",
    "    for j in bins:\n",
    "        frame.loc[:, xs] = j\n",
    "        dframe = xgb.DMatrix(frame)\n",
    "        par_dep_i = pd.DataFrame(model.predict(dframe))\n",
    "        par_dep_j = par_dep_i.mean()[0]\n",
    "        par_dep_frame = par_dep_frame.append({xs:j,\n",
    "                                              'partial_dependence': par_dep_j}, \n",
    "                                              ignore_index=True)\n",
    "        \n",
    "    # return input frame to original cached state    \n",
    "    frame.loc[:, xs] = col_cache\n",
    "\n",
    "    return par_dep_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate partial dependence for the most important input variables in the GBM\n",
    "The partial dependence for `LIMIT_BAL` can be seen to decrease as credit balance limits increase. This finding is aligned with expectations that the model predictions will be monotonically decreasing with increasing `LIMIT_BAL` and parsimonious with well-known business practices in credit lending. Partial dependence for other important values is displayed in plots further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "par_dep_LIMIT_BAL = par_dep('LIMIT_BAL', test[X], xgb_model) # calculate partial dependence for LIMIT_BAL\n",
    "par_dep_AGE = par_dep('AGE', test[X], xgb_model)             # calculate partial dependence for AGE\n",
    "par_dep_EDUCATION = par_dep('EDUCATION', test[X], xgb_model) # calculate partial dependence for EDUCATION\n",
    "\n",
    "# display partial dependence for LIMIT_BAL\n",
    "par_dep_LIMIT_BAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for finding percentiles of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICE can be calculated for any row in the training or test data, but without intimate knowledge of a data source it can be difficult to know where to apply ICE. Calculating and analyzing ICE curves for every row of training and test data set can be overwhelming, even for the example credit card default dataset. One place to start with ICE is to calculate ICE curves at every decile of predicted probabilities in a dataset, giving an indication of local prediction behavior across the dataset. The function below finds and returns the row indices for the maximum, minimum, and deciles of one column in terms of another -- in this case, the model predictions (`p_DEFAULT_NEXT_MONTH`) and the row identifier (`ID`), respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_percentile_dict(yhat, id_, frame):\n",
    "\n",
    "    \"\"\" Returns the percentiles of a column, yhat, as the indices based on \n",
    "        another column id_.\n",
    "    \n",
    "    Args:\n",
    "        yhat: Column in which to find percentiles.\n",
    "        id_: Id column that stores indices for percentiles of yhat.\n",
    "        frame: Pandas DataFrame containing yhat and id_. \n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of percentile values and index column values.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create a copy of frame and sort it by yhat\n",
    "    sort_df = frame.copy(deep=True)\n",
    "    sort_df.sort_values(yhat, inplace=True)\n",
    "    sort_df.reset_index(inplace=True)\n",
    "    \n",
    "    # find top and bottom percentiles\n",
    "    percentiles_dict = {}\n",
    "    percentiles_dict[0] = sort_df.loc[0, id_]\n",
    "    percentiles_dict[99] = sort_df.loc[sort_df.shape[0]-1, id_]\n",
    "\n",
    "    # find 10th-90th percentiles\n",
    "    inc = sort_df.shape[0]//10\n",
    "    for i in range(1, 10):\n",
    "        percentiles_dict[i * 10] = sort_df.loc[i * inc,  id_]\n",
    "\n",
    "    return percentiles_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find some percentiles of yhat in the test data\n",
    "The values for `ID` that correspond to the maximum, minimum, and deciles of `p_DEFAULT_NEXT_MONTH` are displayed below. ICE will be calculated for the rows of the test dataset associated with these `ID` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge GBM predictions onto test data\n",
    "yhat_test = pd.concat([test.reset_index(drop=True), pd.DataFrame(xgb_model.predict(dtest))], axis=1)\n",
    "yhat_test = yhat_test.rename(columns={0:'p_DEFAULT_NEXT_MONTH'})\n",
    "\n",
    "# find percentiles of predictions\n",
    "percentile_dict = get_percentile_dict('p_DEFAULT_NEXT_MONTH', 'ID', yhat_test)\n",
    "\n",
    "# display percentiles dictionary\n",
    "# ID values for rows\n",
    "# from lowest prediction \n",
    "# to highest prediction\n",
    "percentile_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ICE curve values\n",
    "ICE values represent a model's prediction for a row of data while an input variable of interest is varied across its domain. The values of the input variable are chosen to match the values at which partial dependence was calculated earlier, and ICE is calculated for the top three most important variables and for rows at each percentile of the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retreive bins from original partial dependence calculation\n",
    "bins_LIMIT_BAL = list(par_dep_LIMIT_BAL['LIMIT_BAL'])\n",
    "bins_AGE = list(par_dep_AGE['AGE'])\n",
    "bins_EDUCATION = list(par_dep_EDUCATION['EDUCATION'])\n",
    "\n",
    "# for each percentile in percentile_dict\n",
    "# create a new column in the par_dep frame \n",
    "# representing the ICE curve for that percentile\n",
    "# and the variables of interest\n",
    "for i in sorted(percentile_dict.keys()):\n",
    "    \n",
    "    col_name = 'Percentile_' + str(i)\n",
    "    \n",
    "    # ICE curves for LIMIT_BAL across percentiles at bins_LIMIT_BAL intervals\n",
    "    par_dep_LIMIT_BAL[col_name] = par_dep('LIMIT_BAL', \n",
    "                                          test[test['ID'] == int(percentile_dict[i])][X], \n",
    "                                          xgb_model, \n",
    "                                          bins=bins_LIMIT_BAL)['partial_dependence']\n",
    "    \n",
    "    # ICE curves for AGE across percentiles at bins_AGE intervals\n",
    "    par_dep_AGE[col_name] = par_dep('AGE', \n",
    "                                    test[test['ID'] == int(percentile_dict[i])][X],  \n",
    "                                    xgb_model, \n",
    "                                    bins=bins_AGE)['partial_dependence']\n",
    "\n",
    "    # ICE curves for EDUCATION across percentiles at bins_EDUCATION intervals\n",
    "    par_dep_EDUCATION[col_name] = par_dep('EDUCATION', \n",
    "                                          test[test['ID'] == int(percentile_dict[i])][X],  \n",
    "                                          xgb_model, \n",
    "                                          bins=bins_EDUCATION)['partial_dependence']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display partial dependence and ICE for `LIMIT_BAL`\n",
    "Partial dependence and ICE values for rows at the minimum, maximum and deciles (0%, 10%, 20%, ..., 90%, 99%) of predictions for `DEFAULT_NEXT_MONTH` and at the values of `LIMIT_BAL` used for partial dependence are shown here. Each column of ICE values will be a curve in the plots below. ICE values represent a prediction for a row of test data, at a percentile of interest noted in the column name above, and setting `LIMIT_BAL` to the value of `LIMIT_BAL` at right. Notice that monotonic decreasing prediction behavior for `LIMIT_BAL` holds at each displayed percentile of predicted `DEFAULT_NEXT_MONTH`, helping to validate that the trained GBM predictions are monotonic for `LIMIT_BAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "par_dep_LIMIT_BAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting partial dependence and ICE to validate and explain monotonic behavior\n",
    "\n",
    "Overlaying partial dependence onto ICE in a plot is a convenient way to validate and understand both global and local monotonic behavior. Plots of partial dependence curves overlayed onto ICE curves for several percentiles of predictions for `DEFAULT_NEXT_MONTH` are used to validate monotonic behavior, describe the GBM model mechanisms, and to compare the most extreme GBM behavior with the average GBM behavior in the test data. Partial dependence and ICE plots are displayed for the three most important variables in the GBM: `LIMIT_BAL`, `AGE`, and `EDUCATION`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to plot partial dependence and ICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_par_dep_ICE(xs, par_dep_frame):\n",
    "\n",
    "    \n",
    "    \"\"\" Plots ICE overlayed onto partial dependence for a single variable.\n",
    "    \n",
    "    Args: \n",
    "        xs: Name of variable for which to plot ICE and partial dependence.\n",
    "        par_dep_frame: Name of Pandas DataFrame containing ICE and partial\n",
    "                       dependence values.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # plot ICE curves\n",
    "    par_dep_frame.drop('partial_dependence', axis=1).plot(x=xs, \n",
    "                                                          colormap='gnuplot',\n",
    "                                                          ax=ax)\n",
    "\n",
    "    # overlay partial dependence, annotate plot\n",
    "    par_dep_frame.plot(title='Partial Dependence and ICE for ' + str(xs),\n",
    "                       x=xs, \n",
    "                       y='partial_dependence',\n",
    "                       style='r-', \n",
    "                       linewidth=3, \n",
    "                       ax=ax)\n",
    "\n",
    "    # add legend\n",
    "    _ = plt.legend(bbox_to_anchor=(1.05, 0),\n",
    "                   loc=3, \n",
    "                   borderaxespad=0.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial dependence and ICE plot for `LIMIT_BAL`\n",
    "The monotonic prediction behavior displayed in the partial dependence, and ICE tables for `LIMIT_BAL` is also visible in this plot. Monotonic decreasing behavior is evident at every percentile of predictions for `DEFAULT_NEXT_MONTH`. Most percentiles of predictions show that sharper decreases in probability of default occur when `LIMIT_BAL` increases just slightly from its lowest values in the test set. However, for the custumers that are most likely to default according to the GBM model, no increase in `LIMIT_BAL` has a strong impact on probabilitiy of default. As mentioned previously, the displayed relationship between credit balance limits and probablility of default is not uncommon in credit lending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_par_dep_ICE('LIMIT_BAL', par_dep_LIMIT_BAL) # plot partial dependence and ICE for LIMIT_BAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial dependence and ICE plot for `AGE`\n",
    "Monotonic increasing prediction behavior for `AGE` is displayed for all percentiles of model predictions. Predition behavior is different at different deciles, but not abnormal or vastly different from the average prediction behavior represented by the red partial dependence curve, except for the customer most likely to default. For this high-risk customer, changes in AGE do not have a large impact on their probability of default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_par_dep_ICE('AGE', par_dep_AGE) # plot partial dependence and ICE for AGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial dependence and ICE plot for `EDUCATION`\n",
    "Monotonic increasing prediction behavior for `EDUCATION` is also displayed for all percentiles. This mild increase in probability of default as education increases could be related to more educated customers taking on more debt than less educated customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_par_dep_ICE('EDUCATION', par_dep_EDUCATION) # plot partial dependence and ICE for EDUCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "In this notebook, a highly transparent, nonlinear, monotonic GBM classifier was trained to predict credit card defaults and the monotonic behavior of the classifier was analyzed and validated. To do so, Pearson correlation between each input and the target was used to determine the direction for monotonicity constraints for each input variable in the XGBoost classifier. GBM variable importance, partial dependence, and ICE were calculated, plotted, and compared to one another, domain knowledge, and reasonable expectations. These techniques should generalize well for many types of business and research problems, enabling you to train a monotonic GBM model and analyze, validate, and explain it to your colleagues, bosses, and potentially, external regulators. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
